{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Report After T5 Model Training"
      ],
      "metadata": {
        "id": "6XZ6V6ENk4-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -1\n",
        "- **Get the data from bbc news Zip file**"
      ],
      "metadata": {
        "id": "iiuFn9fnlLyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -2\n",
        "- **Data Visulaization and Processing**\n",
        "  - Get some insights in data"
      ],
      "metadata": {
        "id": "6g5z9Whelh8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -3\n",
        "- **Import Some Necessary Libaray**\n",
        "  - Here I include some library which i not use but due to correcting and errors try to use everything"
      ],
      "metadata": {
        "id": "ccgZDGYZl5ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -4\n",
        "- ## Custom Dataset Class for News Summarization with Tokenization and Encoding\n",
        "\n",
        "1. **Initialization**:\n",
        "   - The `NewsSummaryDataset` class serves to process the data for news summarization.\n",
        "   - It takes input data in the form of a Pandas DataFrame, a tokenizer (in this case, T5Tokenizer), and sets maximum token lengths for text and summary.\n",
        "   \n",
        "2. **Data Processing**:\n",
        "   - The `__getitem__` method retrieves the text and summary data from the DataFrame.\n",
        "   - The text and summary data are tokenized and preprocessed using the provided `tokenizer`. The data is converted to token IDs and attention masks.\n",
        "   \n",
        "3. **Label Creation**:\n",
        "   - For the summary, special tokens and padding are added. It creates token labels by masking the padded parts of the summary text, allowing the model to ignore them during the training process.\n",
        "   \n",
        "4. **Returned Dictionary**:\n",
        "   - It constructs and returns a dictionary containing the following key-value pairs:\n",
        "       - `'text'`: Original text content.\n",
        "       - `'summary'`: Original summary content.\n",
        "       - `'text_input_ids'`: Flattened token IDs of the tokenized text.\n",
        "       - `'text_attention_mask'`: Flattened attention mask for the tokenized text.\n",
        "       - `'labels'`: Flattened token labels (with padded areas masked).\n",
        "       - `'labels_attention_mask'`: Flattened attention mask for tokenized summaries.\n",
        "\n",
        "5. **Length Information**:\n",
        "   - The `__len__` method returns the length of the dataset, indicating the number of records or samples present in the input DataFrame.\n",
        "\n",
        "This code essentially prepares the dataset for training a summarization model by processing and organizing the data into a format suitable for the model's input requirements."
      ],
      "metadata": {
        "id": "2z6pAcbil8-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -5"
      ],
      "metadata": {
        "id": "zQ3YDzYwrgRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LightningModule Definition for T5-based News Summarization\n",
        "- This LightningModule script outlines a PyTorch Lightning implementation for a T5 model used in news summarization. Here's a breakdown of this code section:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - The `NewsSummaryModel` class initializes the LightningModule, setting up the T5 model (`T5ForConditionalGeneration`) from the `'t5-base'` checkpoint and enabling the return of a dictionary from the model's forward pass.\n",
        "\n",
        "2. **Forward Method**:\n",
        "   - The `forward` method is designed to receive input arguments for `input_ids`, `attention_mask`, `decoder_attention_mask`, and optional `labels`. It performs a forward pass through the T5 model with these inputs.\n",
        "   - The method returns the loss and logits calculated from the model output.\n",
        "\n",
        "3. **Training Step**:\n",
        "   - The `training_step` method is meant to define a single step within the training loop. It extracts the necessary input data from the batch dictionary provided by the DataLoader.\n",
        "   - It invokes the `forward` method to perform the forward pass, computes the loss, and logs the training loss for visualization using `self.log`.\n",
        "\n",
        "4. **Configure Optimizers**:\n",
        "   - The `configure_optimizers` method specifies the optimizer used for training. In this case, it employs the `AdamW` optimizer for updating the model parameters.\n",
        "\n",
        "This script provides the necessary elements to configure the T5 model within a PyTorch Lightning setup for training on the news summarization dataset."
      ],
      "metadata": {
        "id": "OCTqKd43nHlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -6"
      ],
      "metadata": {
        "id": "-H9ptc7jrdXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lightning Data Module Setup for News Summary Dataset\n",
        "- This code defines a Lightning Data Module called `NewsSummaryDataModule`, used for organizing and handling the datasets for news summarization. Here's a breakdown of this script:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - The `__init__` method sets up the parameters for this data module. It takes `train_df` and `test_df` DataFrames, a tokenizer, and other optional parameters such as `batch_size`, `text_max_token_len`, and `summary_max_token_len`.\n",
        "\n",
        "2. **Setup Method**:\n",
        "   - The `setup` method is used to prepare the datasets. In this case, it's creating instances of `NewsSummaryDataset` for both the training and testing sets.\n",
        "   - It's important to note that the training and testing datasets are preprocessed using the `NewsSummaryDataset` class instantiated with the provided parameters.\n",
        "\n",
        "3. **Train Dataloader**:\n",
        "   - The `train_dataloader` method creates and returns a `DataLoader` for the training dataset. It uses the `train_dataset` initialized in the `setup` method.\n",
        "   - The `DataLoader` is configured with the specified `batch_size`, set to shuffle the data, and allows a specified number of workers for loading the data in parallel.\n",
        "\n",
        "This script helps organize and load data into the PyTorch Lightning training system by setting up the necessary data modules for the training process."
      ],
      "metadata": {
        "id": "87iNi0WrrllM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -7\n",
        "- Initialization and Training Configuration for the News Summary Model"
      ],
      "metadata": {
        "id": "sOUdnHA2sLnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -8\n",
        "- Train the model"
      ],
      "metadata": {
        "id": "66uHhEIfsVzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -9\n",
        "- `summarize_text`, generates a summary from the input text using a pre-trained model (in this case, a T5 model). Below is the breakdown of this code:\n",
        "\n",
        "1. **Text Encoding**:\n",
        "   - The provided `text` input is tokenized and encoded using the specified tokenizer.\n",
        "   - The text is converted into tokens, with settings like `max_length`, `padding`, and `truncation` controlled by the tokenizer.\n",
        "   - The `return_tensors` parameter is set to return PyTorch tensors.\n",
        "\n",
        "2. **Generating the Summary**:\n",
        "   - The encoded input text is passed to the T5 model to generate a summary.\n",
        "   - The `generate` method from `model.model` is utilized to create a summary.\n",
        "   - It specifies various parameters such as `max_length` for the maximum length of the generated text, `num_beams` for the number of beams in beam search, `length_penalty` to modify the length of the generated output, and `early_stopping` to control whether to stop the generation early.\n",
        "   - The `generate` method returns generated token IDs for the summary.\n",
        "\n",
        "3. **Decoding and Joining**:\n",
        "   - The generated token IDs are decoded using the tokenizer, skipping special tokens and cleaning up tokenization spaces.\n",
        "   - The decoded tokens are joined together to form the final summarized text, which is then returned.\n",
        "\n",
        "This function combines tokenization, model inference, and decoding to generate a summary of the provided text input using the pre-trained T5 model."
      ],
      "metadata": {
        "id": "7lEWH-nOseAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step -10\n",
        "- ROUGE scores for the summarization model"
      ],
      "metadata": {
        "id": "nxn1pR2HshO0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) scores for the model-generated summaries compared to the reference summaries present in the test dataset. Here's a breakdown of what this code does:\n",
        "\n",
        "1. **Initialization**:\n",
        "   - It creates a RougeScorer object to calculate ROUGE scores. The scorer is configured to compute ROUGE-1, ROUGE-2, and ROUGE-L scores, and it uses stemming.\n",
        "\n",
        "2. **ROUGE Scores Calculation**:\n",
        "   - It iterates through each row in the `test_df` DataFrame to get the reference summary and the generated summary.\n",
        "   - For each row, it uses the `summarizeText` function (assuming it's been previously defined) to generate a summary from the article in that row.\n",
        "   - It then computes ROUGE scores by comparing the generated summary (`model_summary`) with the reference summary (`reference_summary`) for each row.\n",
        "\n",
        "3. **Aggregation of Scores**:\n",
        "   - For each ROUGE score (ROUGE-1, ROUGE-2, and ROUGE-L), the code aggregates the scores across all the examples in the test dataset.\n",
        "\n",
        "4. **Average ROUGE Scores**:\n",
        "   - It calculates the average ROUGE scores for all the examples in the test set for each ROUGE metric.\n",
        "\n",
        "5. **Displaying ROUGE Scores**:\n",
        "   - It prints out the overall ROUGE scores for ROUGE-1, ROUGE-2, and ROUGE-L.\n",
        "\n",
        "This script essentially evaluates the model's summarization quality by computing ROUGE scores for the generated summaries compared to the provided reference summaries in the test dataset. Adjustments may be required depending on the actual structure and content of your `test_df` and the implementation of the `summarizeText` function."
      ],
      "metadata": {
        "id": "dtxCIQk6s4do"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11"
      ],
      "metadata": {
        "id": "ROJ1_Jkzs-ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Performance Metrics Summary:**\n",
        "\n",
        "- **ROUGE-1:** Achieved a score of 0.6012, signifying a strong match in single words between the model-generated and human-written summaries.\n",
        "\n",
        "- **ROUGE-2:** Scored 0.5220, indicating a good similarity in two-word sequences between the model-generated and reference summaries.\n",
        "\n",
        "- **ROUGE-L:** Obtained a score of 0.4470, representing moderate alignment in longer sequences between the model-generated and human-written summaries.\n"
      ],
      "metadata": {
        "id": "NNXBOuP4umFD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lit0HPmuwkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}